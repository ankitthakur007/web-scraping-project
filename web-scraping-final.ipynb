{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the data of world's top 500 companies by `market capitalization`\n",
    "\n",
    "![image](https://i.imgur.com/1UjHN2u.jpeg)\n",
    "\n",
    "All the data scientists/data analysts doing research or analysis need some **Data** the begin their research with.Now the question that comes to our mind is \"How the data is being prepared, from where we can get this much data, and How to extract the exact data that we need?\n",
    "\n",
    "The answer is,\n",
    "> There are sevaral ways to collect this data and one of them is `Web Scraping`, web scraping: `\"Web scraping is a technique used to extract structured data from websites containing unstructured data through an automated process.\"`.\n",
    "\n",
    "\n",
    "For web scraping some tools are available there and Python comes to your rescue by provinding you with some great libraries which help to make your our work easier,these libraries help to automate our work.\n",
    "> These libraries are:`requests`, `lxml`, `BeautifulSoup`, `scrapy`, `selinium`.\n",
    "\n",
    "Here in this notebook I've used `requests` and `BeautifulSoup` libraries to scrape the data from the website [value.today](https://www.value.today/world/world-top-500-companies)\n",
    "\n",
    "Value.Today is a software analytics company which provides World's Top Corporate Companies Information, Corporate Companies Information, Financial Data of Company and World Financial News."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that I've created in this notebook will be beneficial for the comparative study of these 500 companies. You can easily study about the Rise and Falls of these companies in from each 6 months since 2020.\n",
    "\n",
    "### Here is an outline for the full journey of `Web-Scraping`\n",
    "* First of all we need to import all usefull libraries which we arte going to use in this entire notebook\n",
    "* Parsing all the `html data` by using `requests` and `BeautifulSoup` libraries\n",
    "* Start finding all the usefull `tags` and data inside those tags\n",
    "* After collecting all the data we than create a `dataframe` using pandas library, and also make a `CSV file` to save all the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "* All website generally does not allow to scrape their data, some wbsites allow to use their data for reaserch and study purpose only.\n",
    "* Some websites provides their data in CSV format allowing you to scrape their data with [REST API](https://www.redhat.com/en/topics/api/what-is-a-rest-api)\n",
    "* Before scraping any website, we should look for a terms and conditions page to see if there are explicit rules about scraping. If there are, we should follow them. If there are not, then it becomes more of a judgement call.\n",
    "\n",
    "\n",
    "* You can run the cells by clikicking the `Run` button available in the toolbar, or you can just press `Shift + Enter` to run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"thakubhai-007/web-scraping-final\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Uploading additional files...\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/thakubhai-007/web-scraping-final\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/thakubhai-007/web-scraping-final'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=\"web-scraping-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the webpage with the `requests` library\n",
    "\n",
    "In this section we will import all the libraries which we are going to use in this entire notebook and then we import the webpage using the requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet\n",
    "!pip install beautifulsoup4 --upgrade --quiet\n",
    "!pip install pandas --upgrade --quiet\n",
    "!pip install requests --upgrade --quiet\n",
    "import pandas as pd\n",
    "import jovian\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the webpage we are going to scrape, It is always usefull to put our link in a variable, because if sometimes you need to change the link then you have to go throught the enite notebook, else if you put your link in a variable you can just make change in the variable and it will work for the entire notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_url = \"https://www.value.today/world/world-top-500-companies\"\n",
    "response = requests.get(topic_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status code for the page that we have downloaded above. \n",
    "\n",
    "The status code tells us if the webpage is suitable for scraping or not. A successfull status code must lies between 200 and 299. You may find the usefull HTTPS status code  [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the status code\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our page content in a varibale and check the length of the enitre HTML codes available on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1559705"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(page_content) # Checking the lenght of the page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the length of the page_content is more than the 1.5 million, so this means that the webpage we have imported has too much data with itself. Let's have a look on some of the page_content below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\" prefix=\"content: http://purl.org/rss/1.0/modules/content/  dc: http://purl.org/dc/terms/  foaf: http://xmlns.com/foaf/0.1/  og: http://ogp.me/ns#  rdfs: http://www.w3.org/2000/01/rdf-schema#  schema: http://schema.org/  sioc: http://rdfs.org/sioc/ns#  sioct: http://rdfs.org/sioc/types#  skos: http://www.w3.org/2004/02/skos/core#  xsd: http://www.w3.org/2001/XMLSchema# \">\\n  <head>\\n    <meta charset=\"utf-8\"/>\\n<script async src=\"//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\"></script>\\n<script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:\"ca-pub-2407955258669770\",enable_page_level_ads:true});</script><script>window.google_analytics_uacct=\"UA-121331115-1\";(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,\"script\",\"https://www'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look on the first 1000 characters of the html codes that has been written for the web page\n",
    "page_content[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell output you can see the texts that has been written in the [HTML](https://www.quackit.com/html/examples/html_text_examples.cfm) , "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the HTML source code using BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code below we actually create a copy of the webpage that we have downloaded above, The folowing copy you can find in the notebook manu go to:\n",
    "> File/Open: that will open in a new tab and there you will find an html file with the name [word's-top-500-companies-by-narket-capitalization.html](https://hub.binder.jovian.ml/user/thakubhai-007/api-git-0b8226b-976aef155822_37-7ggm32d3/view/word's-top-500-companies-by-narket-capitalization.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word's-top-500-companies-by-narket-capitalization.html\", 'w') as file:\n",
    "    file.write(page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to read the html that we have created above to extract the data that we needed from the web page.\n",
    "with open(\"word's-top-500-companies-by-narket-capitalization.html\", 'r') as f:\n",
    "    html_source = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making it a BeautifulSoup object so that we can use BS4 library for our use, and then to verify if it is correct check the type of doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = BeautifulSoup(html_source) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the title tag and then have a look on the page title\n",
    "> Tag a new word, I've used here, I'll explain it in next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>World Top 500 Companies by Market Capitalization as on Jan 1st, 2020</title>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag = doc.title\n",
    "title_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'World Top 500 Companies by Market Capitalization as on Jan 1st, 2020'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"thakubhai-007/web-scraping-final\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Uploading additional files...\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/thakubhai-007/web-scraping-final\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/thakubhai-007/web-scraping-final'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before going further, first save our work because we are doing all this on an online platform\n",
    "jovian.commit(project=\"web-scraping-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exracting information form the webpage with `BeautifulSoup` library\n",
    "\n",
    "![image](https://i.imgur.com/fUH2KAg.gif)\n",
    "\n",
    "All the data on a webpage is actually written in [HTML(Hyper Texting Markup Language)](https://devdocs.io/html/), this language uses some tags to save all the information for a webpage. So, in this section we will try to fetch all the useful tags and  information which we needed for our project.\n",
    "\n",
    "You may find the tags on each we page with foolowing process:\n",
    "> Right click on the page or on any text that you want to check the tag or source code, then click on the `inspect`, that will open in a side window of your browser, sometimes it opens in bottom or left or right side it depends on the browser's settings.\n",
    "Here is a sample picture of the source code. \n",
    "![Source code image](https://i.imgur.com/japh8JV.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_block_tag = doc.find_all('div', {'class':'row well views-row'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(company_block_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we try to find the tag and name for the company\n",
    "companies_name = []\n",
    "for tag in company_block_tag:\n",
    "        company_name_tag = tag.find('div',{'class':'views-field views-field-title col-sm-12 clearfix'})\n",
    "        companies_name.append(company_name_tag.find('a').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SAUDI ARABIAN OIL COMPANY (Saudi Aramco)', 'APPLE', 'MICROSOFT CORPORATION', 'ALPHABET', 'AMAZON.COM']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look on the top 5 company names if we have extracted it in right way. And also check the length of the companies name if we have got all the names available on the webpage\n",
    "print(companies_name[:5])\n",
    "len(companies_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, We have extracted exactly 500 companies name, `We got our first mile-stone in our journey`,\n",
    "Let's write few separte codes for extracting data about `Rank`, `Headquarter Location`, `CEO` etc, that we need to save for our study purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = []\n",
    "for tag in company_block_tag:\n",
    "    rank_tag = tag.find('div', {'class':'views-field views-field-field-world-rank-jan-2020 clearfix col-sm-12'})\n",
    "    rank.append(int(rank_tag.find('span').text))\n",
    "\n",
    "hq_location = []\n",
    "for tag in company_block_tag:\n",
    "    hq_tag = tag.find('div', {'class':'views-field views-field-field-headquarters-of-company clearfix col-sm-12'})\n",
    "    hq_location.append(hq_tag.find('span').text)\n",
    "\n",
    "ceo_name = []\n",
    "for tag in company_block_tag:\n",
    "    ceo_tag = tag.find('div', {'class':'views-field views-field-field-ceo clearfix col-sm-12'})\n",
    "    try:\n",
    "        ceo = ceo_tag.find('span', {'class':'field-content'})\n",
    "        ceo_name.append(ceo.text)\n",
    "    except AttributeError:\n",
    "        ceo_name.append(None)\n",
    "        \n",
    "market_cap = []\n",
    "for tag in company_block_tag:\n",
    "    market_cap_tag = tag.find('div', {'class':'views-field views-field-field-market-value-jan-2020 clearfix col-sm-12'})\n",
    "    try:\n",
    "        market_cap_value = market_cap_tag.find('span', {'class':'field-content'})\n",
    "        market_cap.append(market_cap_value.text)\n",
    "    except AttributeError:\n",
    "        market_cap.append(None)\n",
    "        \n",
    "total_employee = []\n",
    "for tag in company_block_tag:\n",
    "    employee_tag = tag.find('div',{'class':'views-field views-field-field-employee-count clearfix col-sm-12'})\n",
    "    try:\n",
    "        total_employee.append(employee_tag.find('span').text)\n",
    "    except AttributeError:\n",
    "        total_employee.append(None)\n",
    "        \n",
    "sectors = []\n",
    "for tag in company_block_tag:\n",
    "    sector_tag = tag.find('div', {'class':'views-field views-field-field-company-category-primary clearfix col-sm-12'})\n",
    "    sectors.append(sector_tag.find('span').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 500 500 500 500 500\n"
     ]
    }
   ],
   "source": [
    "# Have a look if all the data is exactly same in counting for all 500 companies.\n",
    "print(len(rank),len(ceo_name),len(market_cap),len(hq_location), len(total_employee), len(sectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the URL for the company page\n",
    "base_url = 'https://value.today'\n",
    "url = []\n",
    "for tag in company_block_tag:\n",
    "    url_tag = tag.find('div',{'class':'views-field views-field-title col-sm-12 clearfix'})\n",
    "    url.append(base_url + url_tag.find('a')['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse all the information to a `CSV` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a pandas dataframe first we save all the information in a dictionary that will make a dataframe much faster in comparision to other known method. After that we than create a `CSV` file to download it locally or for the reasearch purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_name_dict = {\n",
    "    'Name': companies_name,\n",
    "    'Rank': rank,\n",
    "    'Headquarter': hq_location,\n",
    "    'CEO': ceo_name,\n",
    "    'Market Capitalization': market_cap,\n",
    "    'Total No. Of Employee': total_employee,\n",
    "    'Sectors': sectors,\n",
    "    'url': url\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_df = pd.DataFrame(companies_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Headquarter</th>\n",
       "      <th>CEO</th>\n",
       "      <th>Market Capitalization</th>\n",
       "      <th>Total No. Of Employee</th>\n",
       "      <th>Sectors</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAUDI ARABIAN OIL COMPANY (Saudi Aramco)</td>\n",
       "      <td>1</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>Amin H. Al-Nasser</td>\n",
       "      <td>1898.10 Billion USD</td>\n",
       "      <td>79,000</td>\n",
       "      <td>Energy, Oil and Gas, Chemicals, Oil Refining, ...</td>\n",
       "      <td>https://value.today/company/saudi-arabian-oil-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APPLE</td>\n",
       "      <td>2</td>\n",
       "      <td>USA</td>\n",
       "      <td>Tim Cook</td>\n",
       "      <td>1323.00 Billion USD</td>\n",
       "      <td>147,000</td>\n",
       "      <td>Technology, Mobiles &amp; Accessories, Electronics...</td>\n",
       "      <td>https://value.today/company/apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MICROSOFT CORPORATION</td>\n",
       "      <td>3</td>\n",
       "      <td>USA</td>\n",
       "      <td>Satya Nadella</td>\n",
       "      <td>1215.00 Billion USD</td>\n",
       "      <td>156,439</td>\n",
       "      <td>Technology, Software and IT, Laptops, Video Ga...</td>\n",
       "      <td>https://value.today/company/microsoft-corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALPHABET</td>\n",
       "      <td>4</td>\n",
       "      <td>USA</td>\n",
       "      <td>Sundar Pichai</td>\n",
       "      <td>943.90 Billion USD</td>\n",
       "      <td>135,301</td>\n",
       "      <td>Technology, Internet or Mobile App Based Busin...</td>\n",
       "      <td>https://value.today/company/alphabet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMAZON.COM</td>\n",
       "      <td>5</td>\n",
       "      <td>USA</td>\n",
       "      <td>Jeff Bezos</td>\n",
       "      <td>941.03 Billion USD</td>\n",
       "      <td>1,298,000</td>\n",
       "      <td>eCommerce, Internet or Mobile App Based Busine...</td>\n",
       "      <td>https://value.today/company/amazon.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>SEVEN &amp; I HOLDINGS</td>\n",
       "      <td>471</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Ryuichi Isaka</td>\n",
       "      <td>32.65 Billion USD</td>\n",
       "      <td>58,165</td>\n",
       "      <td>Consumer Defensive, Retail, Super Markets, Con...</td>\n",
       "      <td>https://value.today/company/seven-i-holdings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>ASSICURAZIONI GENERALI</td>\n",
       "      <td>472</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Philippe Donnet</td>\n",
       "      <td>32.61 Billion USD</td>\n",
       "      <td>72,000</td>\n",
       "      <td>Financial Services, Insurance</td>\n",
       "      <td>https://value.today/company/assicurazioni-gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>AMPHENOL CORPORATION</td>\n",
       "      <td>473</td>\n",
       "      <td>USA</td>\n",
       "      <td>Richard Adam Norwitt</td>\n",
       "      <td>32.53 Billion USD</td>\n",
       "      <td>74,000</td>\n",
       "      <td>Technology, Electronics, Cables and Wires, Ele...</td>\n",
       "      <td>https://value.today/company/amphenol-corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>GENERAL MILLS</td>\n",
       "      <td>474</td>\n",
       "      <td>USA</td>\n",
       "      <td>Jeff Harmening</td>\n",
       "      <td>32.52 Billion USD</td>\n",
       "      <td>40,000</td>\n",
       "      <td>Consumer Defensive, Food Products, FMCG, Dairy...</td>\n",
       "      <td>https://value.today/company/general-mills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>FOMENTO ECONOMICO MEXICANOB. DE C.V. (FEMSA)</td>\n",
       "      <td>475</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>None</td>\n",
       "      <td>32.46 Billion USD</td>\n",
       "      <td>297,073</td>\n",
       "      <td>Consumer Defensive, Non-Alcoholic Beverages, B...</td>\n",
       "      <td>https://value.today/company/fomento-economico-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Name  Rank   Headquarter  \\\n",
       "0        SAUDI ARABIAN OIL COMPANY (Saudi Aramco)     1  Saudi Arabia   \n",
       "1                                           APPLE     2           USA   \n",
       "2                           MICROSOFT CORPORATION     3           USA   \n",
       "3                                        ALPHABET     4           USA   \n",
       "4                                      AMAZON.COM     5           USA   \n",
       "..                                            ...   ...           ...   \n",
       "495                            SEVEN & I HOLDINGS   471         Japan   \n",
       "496                        ASSICURAZIONI GENERALI   472         Italy   \n",
       "497                          AMPHENOL CORPORATION   473           USA   \n",
       "498                                 GENERAL MILLS   474           USA   \n",
       "499  FOMENTO ECONOMICO MEXICANOB. DE C.V. (FEMSA)   475        Mexico   \n",
       "\n",
       "                      CEO Market Capitalization Total No. Of Employee  \\\n",
       "0       Amin H. Al-Nasser   1898.10 Billion USD                79,000   \n",
       "1                Tim Cook   1323.00 Billion USD               147,000   \n",
       "2           Satya Nadella   1215.00 Billion USD               156,439   \n",
       "3           Sundar Pichai    943.90 Billion USD               135,301   \n",
       "4              Jeff Bezos    941.03 Billion USD             1,298,000   \n",
       "..                    ...                   ...                   ...   \n",
       "495         Ryuichi Isaka     32.65 Billion USD                58,165   \n",
       "496       Philippe Donnet     32.61 Billion USD                72,000   \n",
       "497  Richard Adam Norwitt     32.53 Billion USD                74,000   \n",
       "498        Jeff Harmening     32.52 Billion USD                40,000   \n",
       "499                  None     32.46 Billion USD               297,073   \n",
       "\n",
       "                                               Sectors  \\\n",
       "0    Energy, Oil and Gas, Chemicals, Oil Refining, ...   \n",
       "1    Technology, Mobiles & Accessories, Electronics...   \n",
       "2    Technology, Software and IT, Laptops, Video Ga...   \n",
       "3    Technology, Internet or Mobile App Based Busin...   \n",
       "4    eCommerce, Internet or Mobile App Based Busine...   \n",
       "..                                                 ...   \n",
       "495  Consumer Defensive, Retail, Super Markets, Con...   \n",
       "496                      Financial Services, Insurance   \n",
       "497  Technology, Electronics, Cables and Wires, Ele...   \n",
       "498  Consumer Defensive, Food Products, FMCG, Dairy...   \n",
       "499  Consumer Defensive, Non-Alcoholic Beverages, B...   \n",
       "\n",
       "                                                   url  \n",
       "0    https://value.today/company/saudi-arabian-oil-...  \n",
       "1                    https://value.today/company/apple  \n",
       "2    https://value.today/company/microsoft-corporation  \n",
       "3                 https://value.today/company/alphabet  \n",
       "4               https://value.today/company/amazon.com  \n",
       "..                                                 ...  \n",
       "495       https://value.today/company/seven-i-holdings  \n",
       "496  https://value.today/company/assicurazioni-gene...  \n",
       "497   https://value.today/company/amphenol-corporation  \n",
       "498          https://value.today/company/general-mills  \n",
       "499  https://value.today/company/fomento-economico-...  \n",
       "\n",
       "[500 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look on the dataframe we have created\n",
    "companies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all the information to a csv file \n",
    "companies_df.to_csv('companies.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"thakubhai-007/web-scraping-final\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Uploading additional files...\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/thakubhai-007/web-scraping-final\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/thakubhai-007/web-scraping-final'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=\"web-scraping-final\", files=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done by now?\n",
    "> We have Scraped the data availbe on the [web page](https://www.value.today/world/world-top-500-companies), \n",
    "\n",
    "This a very basic and beginner way of scraping. We have collected the data about:\n",
    "* Company name\n",
    "* Company CEO\n",
    "* Company Headquarter\n",
    "* Company Rank on January 2020\n",
    "* Company market capitalization on January 2020\n",
    "* Company URL\n",
    "* Total number of employee in the company\n",
    "* Sectors that the company works\n",
    "\n",
    "\n",
    "Now it's time to go some deeper into `Web_Scraping`, further we will try to collect data from each company's individual page availbale on the same wesite. For example visit the page of [APPLE](https://www.value.today/company/apple)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing some functions for scraping some data from each particular company page\n",
    "\n",
    "Python gives us advantage for writing useful and reusable functions, these functions are nothing but a block of re-usable code to perform a specific task. It helps in code re-usability and modular application design\n",
    "\n",
    "In this section we will write functions for getting information about `componay_names`, `Company_rank`, `Company_market_cap` and other things for different time period, but this time all this data will come form each separate company page so it is not posiible to write codes for each single page, that's why we are going to write functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_url(topic_url):\n",
    "    url = [] # create an empty list to add the urls of each company page.\n",
    "    '''We will write codes for getting the company url, \n",
    "    and check the status code again as we have checked previous'''\n",
    "    response = requests.get(topic_url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Status code :\", response.status_code)\n",
    "        raise Exception('Failed to fetch page content ' + topic_url)\n",
    "    # If the response status code is good enough for our scraping then save all the html text in a variable\n",
    "    page_content = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Then we will try to find the url_tag for each companies page inside the page, and then we will extract the exact url\n",
    "    company_block_tag = page_content.find_all('div', {'class':'row well views-row'})\n",
    "    base_url = 'https://value.today'\n",
    "    for tag in company_block_tag:\n",
    "        url_tag = tag.find('div',{'class':'views-field views-field-title col-sm-12 clearfix'})\n",
    "        url.append(base_url + url_tag.find('a')['href'])\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract all the companies urls from the web page we have chosed for scraping..\n",
    "\n",
    "I have put a special time command just before calling the functions to extract all the urls, this time command is Jupyter Notebook's inbuilt magical command, that will tell us `How much time will it take to execute a single second`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 61.3 ms, total: 1.18 s\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "url_list = get_company_url('https://www.value.today/world/world-top-500-companies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great we have collected all the urls available on the page, and saved into a vairable. Let's check for the first url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://value.today/company/saudi-arabian-oil-company'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_0 = url_list[0]\n",
    "url_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will save top 5 companies url in a variable to test and showing the sample of the data set we are going to create in the following code cells.\n",
    "top_5_companies = url_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a fuction to get all the HTML content for each company's wepage, since we will pick all the data from each single webpage so we will return s BeautifulSoup object from this function. Also we will check the status code in the same functoin as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_page(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Status code:\", response.status_code)\n",
    "        raise Exception('Failed to fetch page content ' + company_url +'!')\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function on our veri first url, that we have checked in one of above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content_0 = get_company_page(url_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I'll write some functions to get some important data of our interest, The data about rank and market capitalization of each company.\n",
    "\n",
    "Let's go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jan_2020_market_cap(soup):\n",
    "    market_value_tag = soup.find('div', {'class':'clearfix col-sm-6 field field--name-field-market-value-jan-2020 field--type-float field--label-above'})\n",
    "    # Getting the div tag for the market_cap, we will then fetch the detail inside it and extract the value \n",
    "    try:\n",
    "        market_value = market_value_tag.find('div', {'class':'field--item'}).text\n",
    "    except AttributeError:\n",
    "        market_value = 'None'\n",
    "    return market_value\n",
    "\n",
    "def jan_2020_rank(soup):\n",
    "    rank_tag = soup.find('div', {'class':'clearfix col-sm-6 field field--name-field-world-rank-jan-2020 field--type-integer field--label-above'})\n",
    "     # Getting the div tag for the rank, we will then fetch the detail inside it and extract the value \n",
    "    try:\n",
    "        rank = rank_tag.find('div', {'class':'field--item'}).text\n",
    "    except AttributeError:\n",
    "        rank = 'None'\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above block of code will catch the data about rank and market capitalization on January 2020, \n",
    "\n",
    "Similarly, I'll write three more block of code same as above for collecting the data for `Rank` and `Market Capitalization` for `July 2020`, `August 2020` and `January 2021`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def july_2020_market_cap(soup):\n",
    "    market_value_tag = soup.find('div',{'class':'clearfix col-sm-6 field field--name-field-market-cap-july-04-2020- field--type-float field--label-above'})\n",
    "    try:\n",
    "        market_value = market_value_tag.find('div', {'class':'field--item'}).text\n",
    "    except AttributeError:\n",
    "        market_value = 'None'\n",
    "    return market_value\n",
    "\n",
    "\n",
    "def july_2020_rank(soup):\n",
    "    rank_tag = soup.find('div', {'class':'clearfix col-sm-6 field field--name-field-world-rank-july-04-2020- field--type-integer field--label-above'})\n",
    "    try:\n",
    "        rank = rank_tag.find('div', {'class':'field--item'}).text\n",
    "    except AttributeError:\n",
    "        rank = 'None'\n",
    "    \n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_2020_market_cap(soup):\n",
    "    market_value_tag = soup.find('div',{'class':'clearfix col-sm-6 field field--name-field-market-cap-aug-22-2020 field--type-float field--label-above'})\n",
    "    try:\n",
    "        market_value = market_value_tag.find('div', {'class':'field--item'}).text\n",
    "    except AttributeError:\n",
    "        market_value = 'None'\n",
    "    return market_value\n",
    "\n",
    "def august_2020_rank(soup):\n",
    "    rank_tag = soup.find('div', {'class':'clearfix col-sm-6 field field--name-field-world-rank-aug-22-2020- field--type-integer field--label-above'})\n",
    "    try:\n",
    "        rank = rank_tag.find('div', {'class':'field--item'}).text     \n",
    "    except AttributeError:\n",
    "        rank = 'None'\n",
    "    \n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jan_2021_market_cap(soup):\n",
    "    market_value_tag = soup.find('div',{'class':'clearfix col-sm-6 field field--name-field-market-value-jan012021 field--type-float field--label-above'})\n",
    "    try:\n",
    "        market_value = market_value_tag.find('div', {'class':'field--item'}).text\n",
    "    except AttributeError:\n",
    "        market_value = 'None'\n",
    "    return market_value\n",
    "\n",
    "def jan_2021_rank(soup):\n",
    "    rank_tag = soup.find('div', {'class':'clearfix col-sm-6 field field--name-field-world-rank-jan012021 field--type-integer field--label-above'})\n",
    "    try:\n",
    "        rank = rank_tag.find('div', {'class':'field--item'}).text\n",
    "    except AttributeError:\n",
    "        rank = 'None'\n",
    "    \n",
    "    return rank \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract some more information about `CEO`, `Logo`, `HQ` and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ceo_name(soup):\n",
    "    ceo_tag = soup.find('div', {'class':'clearfix col-sm-12 field field--name-field-ceo field--type-entity-reference field--label-above'})\n",
    "    # We have picked the CEO tag for the company and below we will extract the name of CEO\n",
    "    try:\n",
    "        ceo_name = ceo_tag.find('a').text\n",
    "    except AttributeError:\n",
    "        ceo_name = 'None'\n",
    "    return ceo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_logo(soup):\n",
    "    logo_tag = soup.find('div', {'class':'clearfix col-sm-12 field field--name-field-company-logo-lc field--type-image field--label-hidden field--item'})\n",
    "    try:\n",
    "        logo = base_url + logo_tag.find('img')['src']\n",
    "    except AttributeError:\n",
    "        logo = 'None'\n",
    "    return logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_url(soup):\n",
    "    company_url_tag = soup.find('div', {'class':'clearfix col-sm-12 field field--name-field-company-website field--type-link field--label-above'})\n",
    "    try:\n",
    "        company_url = company_url_tag.find('a')['href']\n",
    "    except AttributeError:\n",
    "        company_url = 'None'\n",
    "    return company_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_headquarter(soup):\n",
    "    hq_tag = soup.find('div', {'class':'clearfix col-sm-12 field field--name-field-headquarters-of-company field--type-entity-reference field--label-above'})\n",
    "    try:\n",
    "        hq_name = hq_tag.find('div',{'class':'field--item'}).text\n",
    "    except AttributeError:\n",
    "        hq_name = 'None'\n",
    "    return hq_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_name(soup):\n",
    "    h1_tag = soup.find('div', {'class':'field field--name-node-title field--type-ds field--label-hidden field--item'}).find('h1')\n",
    "    company_name = h1_tag.find('a').text.strip()\n",
    "    return company_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's write some function to parse the data to the csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our almost work has been completed, as we have wrote almost all the functions regarding getting all the usefull information, for the website.\n",
    "\n",
    "Let's write a function the extract all the data and parse that data into a list of dictionaries, That will make our work much easier to create a pandas dataframe.\n",
    "\n",
    "> This is a tricky part here, Since we are going to extract data from each single company's webpage, and there are 500 companies, so we may get block from the website company, beacause we are extracting this much data so, what I'll do to protect myself, I'll use time.sleep() function from time library, it will allow our function to take a break for just 1 second(It depends on the argument value of the time.sleep() function) and that is enough for protecting ourself before gettimg block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_companies_data(url):\n",
    "    # Let's create data variable to store data for parse data into a dictionary, that will make easy to create a `pandas` dataframe\n",
    "    soup = get_company_page(url) # Make a BeatifulSoup object with our pre defined function\n",
    "    ceo_name = get_ceo_name(soup)\n",
    "    company_logo = get_company_logo(soup)\n",
    "    company_url = get_company_url(soup)\n",
    "    company_hq = get_company_headquarter(soup)\n",
    "    company_name = get_company_name(soup)\n",
    "    # In all the above 5 variables as you can see we have sotred the data for the CEO, Company_Name, Company_URL, Company_Headquarter and Company_LOGO\n",
    "    # In all the below variables we will store data for their ranks and market capitalization for different time period\n",
    "    rank_on_jan_2020 = jan_2020_rank(soup)\n",
    "    rank_on_july_2020 = july_2020_rank(soup)\n",
    "    rank_on_aug_2020 = august_2020_rank(soup)\n",
    "    rank_on_jan_2021 = jan_2021_rank(soup)\n",
    "    market_cap_on_jan_2020 = jan_2020_market_cap(soup)\n",
    "    market_cap_on_july_2020 = july_2020_market_cap(soup)\n",
    "    market_cap_on_aug_2020 = aug_2020_market_cap(soup)\n",
    "    market_cap_on_jan_2021 = jan_2021_market_cap(soup)\n",
    "    time.sleep(1)\n",
    "    #Let's make a print statement just for ensuring the data is being scraped by our function.\n",
    "    print(\"Required data has been scraped from the page: \" + company_name)\n",
    "    # And now we will make a return statement in the form of a dictionary\n",
    "    return {\n",
    "        'Company Name': company_name,\n",
    "        'CEO': ceo_name,\n",
    "        'Headquarter': company_hq,\n",
    "        'Jan 2020 Rank': rank_on_jan_2020,\n",
    "        'Jan 2020 Market Capitalization': market_cap_on_jan_2020,\n",
    "        'July 2020 Rank': rank_on_july_2020,\n",
    "        'July 2020 Market Capitalization': market_cap_on_july_2020,\n",
    "        'Aug 2020 Rank': rank_on_aug_2020,\n",
    "        'Aug 2020 Market Capitalization': market_cap_on_aug_2020,\n",
    "        'Jan 2021 Rank': rank_on_jan_2021,\n",
    "        'Jan 2021 Market Capitalization': market_cap_on_jan_2021,\n",
    "        'Logo':company_logo,\n",
    "        'URL':company_url\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if it works correctly, and also check the time to execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required data has been scraped from the page: SAUDI ARABIAN OIL COMPANY (Saudi Aramco)\n",
      "Required data has been scraped from the page: APPLE\n",
      "Required data has been scraped from the page: MICROSOFT CORPORATION\n",
      "Required data has been scraped from the page: ALPHABET\n",
      "Required data has been scraped from the page: AMAZON.COM\n",
      "CPU times: user 559 ms, sys: 26.7 ms, total: 586 ms\n",
      "Wall time: 6.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "top_5_companies_data = []\n",
    "for url in top_5_companies:\n",
    "    data = parse_companies_data(url)\n",
    "    top_5_companies_data.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice enough, It took almost around 7 seconds to fetch the data from `5 pages`. Let's have a look at the data for these top 5 companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Company Name': 'SAUDI ARABIAN OIL COMPANY (Saudi Aramco)',\n",
       "  'CEO': 'Amin H. Al-Nasser',\n",
       "  'Headquarter': 'Saudi Arabia',\n",
       "  'Jan 2020 Rank': '1',\n",
       "  'Jan 2020 Market Capitalization': '1,898.100 Billion USD',\n",
       "  'July 2020 Rank': '1',\n",
       "  'July 2020 Market Capitalization': '1,953.180 Billion USD',\n",
       "  'Aug 2020 Rank': '2',\n",
       "  'Aug 2020 Market Capitalization': '1,997.390 Billion USD',\n",
       "  'Jan 2021 Rank': '2',\n",
       "  'Jan 2021 Market Capitalization': '2,051.500 Billion USD',\n",
       "  'Logo': 'https://value.today/sites/default/files/styles/medium/public/2020-05/saudi-aaramco.JPG?itok=aNP5GBsE',\n",
       "  'URL': 'https://www.saudiaramco.com/'},\n",
       " {'Company Name': 'APPLE',\n",
       "  'CEO': 'Tim Cook',\n",
       "  'Headquarter': 'USA',\n",
       "  'Jan 2020 Rank': '2',\n",
       "  'Jan 2020 Market Capitalization': '1,323.000 Billion USD',\n",
       "  'July 2020 Rank': '2',\n",
       "  'July 2020 Market Capitalization': '1,578.000 Billion USD',\n",
       "  'Aug 2020 Rank': '1',\n",
       "  'Aug 2020 Market Capitalization': '2,127.000 Billion USD',\n",
       "  'Jan 2021 Rank': '1',\n",
       "  'Jan 2021 Market Capitalization': '2,256.000 Billion USD',\n",
       "  'Logo': 'https://value.today/sites/default/files/styles/medium/public/2020-05/Apple.jpg?itok=tfuZcdsp',\n",
       "  'URL': 'https://www.apple.com/'},\n",
       " {'Company Name': 'MICROSOFT CORPORATION',\n",
       "  'CEO': 'Satya Nadella',\n",
       "  'Headquarter': 'USA',\n",
       "  'Jan 2020 Rank': '3',\n",
       "  'Jan 2020 Market Capitalization': '1,215.000 Billion USD',\n",
       "  'July 2020 Rank': '3',\n",
       "  'July 2020 Market Capitalization': '1,564.000 Billion USD',\n",
       "  'Aug 2020 Rank': '4',\n",
       "  'Aug 2020 Market Capitalization': '1,612.000 Billion USD',\n",
       "  'Jan 2021 Rank': '3',\n",
       "  'Jan 2021 Market Capitalization': '1,682.000 Billion USD',\n",
       "  'Logo': 'https://value.today/sites/default/files/styles/medium/public/2020-05/Microsoft.jpg?itok=_vYmm6fS',\n",
       "  'URL': 'https://www.microsoft.com/'},\n",
       " {'Company Name': 'ALPHABET',\n",
       "  'CEO': 'Sundar Pichai',\n",
       "  'Headquarter': 'USA',\n",
       "  'Jan 2020 Rank': '4',\n",
       "  'Jan 2020 Market Capitalization': '943.897 Billion USD',\n",
       "  'July 2020 Rank': '5',\n",
       "  'July 2020 Market Capitalization': '1,002.000 Billion USD',\n",
       "  'Aug 2020 Rank': '5',\n",
       "  'Aug 2020 Market Capitalization': '1,073.000 Billion USD',\n",
       "  'Jan 2021 Rank': '5',\n",
       "  'Jan 2021 Market Capitalization': '1,185.000 Billion USD',\n",
       "  'Logo': 'https://value.today/sites/default/files/styles/medium/public/2020-05/Alphabet.jpg?itok=Jj27D9zR',\n",
       "  'URL': 'https://abc.xyz/'},\n",
       " {'Company Name': 'AMAZON.COM',\n",
       "  'CEO': 'Jeff Bezos',\n",
       "  'Headquarter': 'USA',\n",
       "  'Jan 2020 Rank': '5',\n",
       "  'Jan 2020 Market Capitalization': '941.028 Billion USD',\n",
       "  'July 2020 Rank': '4',\n",
       "  'July 2020 Market Capitalization': '1,442.000 Billion USD',\n",
       "  'Aug 2020 Rank': '3',\n",
       "  'Aug 2020 Market Capitalization': '1,645.000 Billion USD',\n",
       "  'Jan 2021 Rank': '4',\n",
       "  'Jan 2021 Market Capitalization': '1,634.000 Billion USD',\n",
       "  'Logo': 'https://value.today/sites/default/files/styles/medium/public/2020-05/amazon.jpg?itok=t0iBs8ya',\n",
       "  'URL': 'https://www.amazon.com/'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_companies_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final command\n",
    "Now it's time to give a final command to collect all the data from those 500 pages, and then we will put that into our `Dataframe`\n",
    "We will put the enitre url_list of 500 URLs into our `parse_companies_data` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required data has been scraped from the page: SAUDI ARABIAN OIL COMPANY (Saudi Aramco)\n",
      "Required data has been scraped from the page: APPLE\n",
      "Required data has been scraped from the page: MICROSOFT CORPORATION\n",
      "Required data has been scraped from the page: ALPHABET\n",
      "Required data has been scraped from the page: AMAZON.COM\n",
      "Required data has been scraped from the page: FACEBOOK\n",
      "Required data has been scraped from the page: ALIBABA GROUP HOLDING\n",
      "Required data has been scraped from the page: BERKSHIRE HATHAWAY\n",
      "Required data has been scraped from the page: TENCENT\n",
      "Required data has been scraped from the page: J P MORGAN CHASE & CO\n",
      "Required data has been scraped from the page: VISA\n",
      "Required data has been scraped from the page: JOHNSON & JOHNSON\n",
      "Required data has been scraped from the page: WALMART\n",
      "Required data has been scraped from the page: SAMSUNG ELECTRONICS\n",
      "Required data has been scraped from the page: BANK OF AMERICA CORPORATION\n",
      "Required data has been scraped from the page: NESTLE AG\n",
      "Required data has been scraped from the page: PROCTER & GAMBLE COMPANY\n",
      "Required data has been scraped from the page: MASTERCARD\n",
      "Required data has been scraped from the page: ICBC\n",
      "Required data has been scraped from the page: EXXON MOBIL CORPORATION\n",
      "Required data has been scraped from the page: TAIWAN SEMICONDUCTOR MANUFACTURING COMPANY\n",
      "Required data has been scraped from the page: AT&T\n",
      "Required data has been scraped from the page: ROCHE HOLDING AG\n",
      "Required data has been scraped from the page: UNITEDHEALTH GROUP\n",
      "Required data has been scraped from the page: THE WALT DISNEY COMPANY\n",
      "Required data has been scraped from the page: INTEL CORPORATION\n",
      "Required data has been scraped from the page: VERIZON COMMUNICATIONS\n",
      "Required data has been scraped from the page: HOME DEPOT\n",
      "Required data has been scraped from the page: LVMH\n",
      "Required data has been scraped from the page: ROYAL DUTCH SHELL\n",
      "Required data has been scraped from the page: COCA-COLA COMPANY\n",
      "Required data has been scraped from the page: MERCK & COMPANY\n",
      "Required data has been scraped from the page: CHEVRON CORPORATION\n",
      "Required data has been scraped from the page: WELLS FARGO & COMPANY\n",
      "Required data has been scraped from the page: PING AN INSURANCE (GROUP) COMPANY OF CHINA\n",
      "Required data has been scraped from the page: CHINA CONSTRUCTION BANK CORPORATION\n",
      "Required data has been scraped from the page: PFIZER\n",
      "Required data has been scraped from the page: NOVARTIS AG\n",
      "Required data has been scraped from the page: COMCAST CORPORATION\n",
      "Required data has been scraped from the page: CISCO SYSTEMS\n",
      "Required data has been scraped from the page: TOYOTA MOTOR CORPORATION\n",
      "Required data has been scraped from the page: KWEICHOW MOUTAI\n",
      "Required data has been scraped from the page: PEPSICO\n",
      "Required data has been scraped from the page: BOEING COMPANY\n",
      "Required data has been scraped from the page: AGRICULTURAL BANK OF CHINA\n",
      "Required data has been scraped from the page: CITI GROUP\n",
      "Required data has been scraped from the page: CHINA MOBILE\n",
      "Required data has been scraped from the page: ORACLE CORPORATION\n",
      "Required data has been scraped from the page: SAP SE\n",
      "Required data has been scraped from the page: L'OREAL\n",
      "Required data has been scraped from the page: ANHEUSER-BUSCH INBEV\n",
      "Required data has been scraped from the page: ADOBE\n",
      "Required data has been scraped from the page: HSBC HOLDINGS\n",
      "Required data has been scraped from the page: NIKE\n",
      "Required data has been scraped from the page: MCDONALD'S CORPORATION\n",
      "Required data has been scraped from the page: MEDTRONIC\n",
      "Required data has been scraped from the page: ABBOTT LABORATORIES\n",
      "Required data has been scraped from the page: BRISTOL-MYERS SQUIBB COMPANY\n",
      "Required data has been scraped from the page: UNILEVER GROUP\n",
      "Required data has been scraped from the page: BANK OF CHINA\n",
      "Required data has been scraped from the page: PETROCHINA COMPANY\n",
      "Required data has been scraped from the page: SALESFORCE.COM\n",
      "Required data has been scraped from the page: NVIDIA CORPORATION\n",
      "Required data has been scraped from the page: TOTAL S.A\n",
      "Required data has been scraped from the page: NETFLIX\n",
      "Required data has been scraped from the page: AMGEN\n",
      "Required data has been scraped from the page: BHP GROUP\n",
      "Required data has been scraped from the page: CHINA MERCHANTS BANK\n",
      "Required data has been scraped from the page: NOVO NORDISK A/S\n",
      "Required data has been scraped from the page: HUAWEI\n",
      "Required data has been scraped from the page: RELIANCE INDUSTRIES\n",
      "Required data has been scraped from the page: PHILIP MORRIS INTERNATIONAL\n",
      "Required data has been scraped from the page: ASTRAZENECA\n",
      "Required data has been scraped from the page: ACCENTURE\n",
      "Required data has been scraped from the page: THERMO FISHER SCIENTIFIC\n",
      "Required data has been scraped from the page: AIA GROUP\n",
      "Required data has been scraped from the page: ABBVIE\n",
      "Required data has been scraped from the page: COSTCO WHOLESALE CORPORATION\n",
      "Required data has been scraped from the page: RAYTHEON TECHNOLOGIES CORPORATION\n",
      "Required data has been scraped from the page: PAYPAL HOLDINGS\n",
      "Required data has been scraped from the page: BP P.L.C.\n",
      "Required data has been scraped from the page: HONEYWELL INTERNATIONAL\n",
      "Required data has been scraped from the page: ELI LILLY AND COMPANY\n",
      "Required data has been scraped from the page: CHINA LIFE INSURANCE COMPANY\n",
      "Required data has been scraped from the page: ASML HOLDING\n",
      "Required data has been scraped from the page: BROADCOM\n",
      "Required data has been scraped from the page: UNION PACIFIC CORPORATION\n",
      "Required data has been scraped from the page: SANOFI\n",
      "Required data has been scraped from the page: TEXAS INSTRUMENTS\n",
      "Required data has been scraped from the page: PROSUS\n",
      "Required data has been scraped from the page: IBM\n",
      "Required data has been scraped from the page: ANT GROUP\n",
      "Required data has been scraped from the page: NEXTERA ENERGY\n",
      "Required data has been scraped from the page: AIRBUS\n",
      "Required data has been scraped from the page: GLAXOSMITHKLINE\n",
      "Required data has been scraped from the page: RIO TINTO GROUP\n",
      "Required data has been scraped from the page: LINDE AG\n",
      "Required data has been scraped from the page: ROYAL BANK OF CANADA\n",
      "Required data has been scraped from the page: TATA CONSULTANCY SERVICES (TCS)\n",
      "Required data has been scraped from the page: LOCKHEED MARTIN CORPORATION\n",
      "Required data has been scraped from the page: VOLKSWAGEN\n",
      "Required data has been scraped from the page: INDITEX (INDUSTRIA DE DISENO TEXTIL)\n",
      "Required data has been scraped from the page: SIEMENS\n",
      "Required data has been scraped from the page: DANAHER CORPORATION\n",
      "Required data has been scraped from the page: STARBUCKS CORPORATION\n",
      "Required data has been scraped from the page: SCHWARZ GROUP\n",
      "Required data has been scraped from the page: ALLIANZ SE\n",
      "Required data has been scraped from the page: CHARTER COMMUNICATIONS\n",
      "Required data has been scraped from the page: PETROLEO BRASILEIRO - PETROBRAS\n",
      "Required data has been scraped from the page: DIAGEO\n",
      "Required data has been scraped from the page: AMERICAN EXPRESS COMPANY\n",
      "Required data has been scraped from the page: GENERAL ELECTRIC COMPANY\n",
      "Required data has been scraped from the page: 3M COMPANY\n",
      "Required data has been scraped from the page: QUALCOMM\n",
      "Required data has been scraped from the page: AMERICAN TOWER CORPORATION (REIT)\n",
      "Required data has been scraped from the page: THE GOLDMAN SACHS GROUP\n",
      "Required data has been scraped from the page: YOUTUBE\n",
      "Required data has been scraped from the page: TORONTO-DOMINION BANK\n",
      "Required data has been scraped from the page: UNITED PARCEL SERVICE (UPS)\n",
      "Required data has been scraped from the page: BRITISH AMERICAN TOBACCO\n",
      "Required data has been scraped from the page: SONY CORPORATION\n",
      "Required data has been scraped from the page: MARS INC\n",
      "Required data has been scraped from the page: COMMONWEALTH BANK OF AUSTRALIA\n",
      "Required data has been scraped from the page: CVS HEALTH CORPORATION\n",
      "Required data has been scraped from the page: HDFC BANK\n",
      "Required data has been scraped from the page: NIPPON TELEGRAPH AND TELEPHONE CORPORATION\n",
      "Required data has been scraped from the page: REYES HOLDINGS\n",
      "Required data has been scraped from the page: U.S. BANCORP\n",
      "Required data has been scraped from the page: GAZPROM\n",
      "Required data has been scraped from the page: LOWE'S COMPANIES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required data has been scraped from the page: SOFTBANK GROUP\n",
      "Required data has been scraped from the page: CHRISTIAN DIOR\n",
      "Required data has been scraped from the page: ALTRIA GROUP\n",
      "Required data has been scraped from the page: NTT DOCOMO\n",
      "Required data has been scraped from the page: SBERBANK OF RUSSIA\n",
      "Required data has been scraped from the page: CHINA PETROLEUM & CHEMICAL CORPORATION (SINOPEC)\n",
      "Required data has been scraped from the page: CSL\n",
      "Required data has been scraped from the page: STATE FARM\n",
      "Required data has been scraped from the page: BOOKING HOLDINGS\n",
      "Required data has been scraped from the page: FIDELITY NATIONAL INFORMATION SERVICES\n",
      "Required data has been scraped from the page: KEYENCE CORPORATION\n",
      "Required data has been scraped from the page: ITAU UNIBANCO HOLDING\n",
      "Required data has been scraped from the page: KERING\n",
      "Required data has been scraped from the page: MORGAN STANLEY\n",
      "Required data has been scraped from the page: CATERPILLAR\n",
      "Required data has been scraped from the page: INGRAM MICRO\n",
      "Required data has been scraped from the page: GILEAD SCIENCES\n",
      "Required data has been scraped from the page: ENEL\n",
      "Required data has been scraped from the page: ENBRIDGE\n",
      "Required data has been scraped from the page: MONDELEZ INTERNATIONAL\n",
      "Required data has been scraped from the page: TESLA\n",
      "Required data has been scraped from the page: BLACKROCK\n",
      "Required data has been scraped from the page: KOCH INDUSTRIES\n",
      "Required data has been scraped from the page: BYTEDANCE\n",
      "Required data has been scraped from the page: FISERV\n",
      "Required data has been scraped from the page: STRYKER CORPORATION\n",
      "Required data has been scraped from the page: MEITUAN DIANPING\n",
      "Required data has been scraped from the page: HERMES INTERNATIONAL\n",
      "Required data has been scraped from the page: DEUTSCHE TELEKOM AG\n",
      "Required data has been scraped from the page: BAYER\n",
      "Required data has been scraped from the page: ROSNEFT OIL COMPANY\n",
      "Required data has been scraped from the page: CIGNA CORPORATION\n",
      "Required data has been scraped from the page: ANTHEM\n",
      "Required data has been scraped from the page: TRUIST FINANCIAL CORPORATION\n",
      "Required data has been scraped from the page: CNOOC LIMITED\n",
      "Required data has been scraped from the page: SAUDI BASIC INDUSTRIES CORPORATION (SABIC)\n",
      "Required data has been scraped from the page: AMBEV\n",
      "Required data has been scraped from the page: NASPERS\n",
      "Required data has been scraped from the page: BNP PARIBAS\n",
      "Required data has been scraped from the page: ESTEE LAUDER COMPANIES\n",
      "Required data has been scraped from the page: TJX COMPANIES\n",
      "Required data has been scraped from the page: BIOGEN\n",
      "Required data has been scraped from the page: ROBERT BOSCH\n",
      "Required data has been scraped from the page: AUTOMATIC DATA PROCESSING (ADP)\n",
      "Required data has been scraped from the page: BECTON DICKINSON AND COMPANY\n",
      "Required data has been scraped from the page: CME GROUP\n",
      "Required data has been scraped from the page: CONOCOPHILLIPS COMPANY\n",
      "Required data has been scraped from the page: BANCO BRADESCO\n",
      "Required data has been scraped from the page: MITSUBISHI UFJ FINANCIAL GROUP\n",
      "Required data has been scraped from the page: BANCO SANTANDER\n",
      "Required data has been scraped from the page: L'AIR LIQUIDE\n",
      "Required data has been scraped from the page: DELOITTE\n",
      "Required data has been scraped from the page: BASF SE\n",
      "Required data has been scraped from the page: WULIANGYE YIBIN\n",
      "Required data has been scraped from the page: THE PNC FINANCIAL SERVICES GROUP\n",
      "Required data has been scraped from the page: VALE\n",
      "Required data has been scraped from the page: CARGILL\n",
      "Required data has been scraped from the page: CHUBB\n",
      "Required data has been scraped from the page: KDDI CORPORATION\n",
      "Required data has been scraped from the page: INTUIT\n",
      "Required data has been scraped from the page: BANK OF NOVA SCOTIA\n",
      "Required data has been scraped from the page: DOMINION ENERGY\n",
      "Required data has been scraped from the page: DUKE ENERGY CORPORATION\n",
      "Required data has been scraped from the page: AXA\n",
      "Required data has been scraped from the page: T-MOBILE US\n",
      "Required data has been scraped from the page: INTUITIVE SURGICAL\n",
      "Required data has been scraped from the page: EQUINOR ASA\n",
      "Required data has been scraped from the page: S&P GLOBAL\n",
      "Required data has been scraped from the page: BLACKSTONE GROUP\n",
      "Required data has been scraped from the page: ESSILOR LUXOTTICA\n",
      "Required data has been scraped from the page: SOUTHERN COMPANY\n",
      "Required data has been scraped from the page: PJSC LUKOIL\n",
      "Required data has been scraped from the page: SAFRAN\n",
      "Required data has been scraped from the page: CANADIAN NATIONAL RAILWAY COMPANY\n",
      "Required data has been scraped from the page: TARGET CORPORATION\n",
      "Required data has been scraped from the page: ADIDAS\n",
      "Required data has been scraped from the page: PRICEWATERHOUSECOOPERS\n",
      "Required data has been scraped from the page: VINCI\n",
      "Required data has been scraped from the page: VMWARE\n",
      "Required data has been scraped from the page: ZOETIS\n",
      "Required data has been scraped from the page: TAKEDA PHARMACEUTICAL COMPANY\n",
      "Required data has been scraped from the page: BOSTON SCIENTIFIC CORPORATION\n",
      "Required data has been scraped from the page: IBERDROLA\n",
      "Required data has been scraped from the page: ALLERGAN\n",
      "Required data has been scraped from the page: PUBLIX\n",
      "Required data has been scraped from the page: THE CHARLES SCHWAB CORPORATION\n",
      "Required data has been scraped from the page: RECRUIT HOLDINGS\n",
      "Required data has been scraped from the page: ZURICH INSURANCE GROUP AG\n",
      "Required data has been scraped from the page: RAYTHEON COMPANY\n",
      "Required data has been scraped from the page: MICRON TECHNOLOGY\n",
      "Required data has been scraped from the page: FAST RETAILING\n",
      "Required data has been scraped from the page: ENTERPRISE PRODUCTS PARTNERS\n",
      "Required data has been scraped from the page: NOVATEK\n",
      "Required data has been scraped from the page: DAIMLER AG\n",
      "Required data has been scraped from the page: JD.COM\n",
      "Required data has been scraped from the page: SIMON PROPERTY GROUP\n",
      "Required data has been scraped from the page: PSBC (POSTAL SAVINGS BANK OF CHINA)\n",
      "Required data has been scraped from the page: WESTPAC BANKING CORPORATION\n",
      "Required data has been scraped from the page: PT BANK CENTRAL ASIA TBK\n",
      "Required data has been scraped from the page: COLGATE-PALMOLIVE COMPANY\n",
      "Required data has been scraped from the page: NORTHROP GRUMMAN CORPORATION\n",
      "Required data has been scraped from the page: HDFC\n",
      "Required data has been scraped from the page: CROWN CASTLE INTERNATIONAL CORPORATION\n",
      "Required data has been scraped from the page: INDUSTRIAL BANK\n",
      "Required data has been scraped from the page: LLOYDS BANKING GROUP\n",
      "Required data has been scraped from the page: HINDUSTAN UNILEVER\n",
      "Required data has been scraped from the page: MIDEA GROUP\n",
      "Required data has been scraped from the page: BROOKFIELD ASSET MANAGEMENT\n",
      "Required data has been scraped from the page: RECKITT BENCKISER GROUP\n",
      "Required data has been scraped from the page: BANK OF COMMUNICATIONS\n",
      "Required data has been scraped from the page: ILLINOIS TOOL WORKS\n",
      "Required data has been scraped from the page: SCHNEIDER ELECTRIC\n",
      "Required data has been scraped from the page: VERTEX PHARMACEUTICALS\n",
      "Required data has been scraped from the page: GREE ELECTRIC APPLIANCES\n",
      "Required data has been scraped from the page: APPLIED MATERIALS\n",
      "Required data has been scraped from the page: ENI S.P.A.\n",
      "Required data has been scraped from the page: CHINA YANGTZE POWER\n",
      "Required data has been scraped from the page: SCHLUMBERGER\n",
      "Required data has been scraped from the page: MARSH & MCLENNAN COMPANIES\n",
      "Required data has been scraped from the page: CSX CORPORATION\n",
      "Required data has been scraped from the page: PROLOGIS\n",
      "Required data has been scraped from the page: ADVANCED MICRO DEVICES (AMD)\n",
      "Required data has been scraped from the page: SK HYNIX\n",
      "Required data has been scraped from the page: GENERAL DYNAMICS CORPORATION\n",
      "Required data has been scraped from the page: ECOLAB\n",
      "Required data has been scraped from the page: DEERE & COMPANY\n",
      "Required data has been scraped from the page: GLOBAL PAYMENTS\n",
      "Required data has been scraped from the page: NATIONAL AUSTRALIA BANK\n",
      "Required data has been scraped from the page: SAUDI TELECOM COMPANY (STC)\n",
      "Required data has been scraped from the page: SERVICENOW\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required data has been scraped from the page: SHERWIN-WILLIAMS COMPANY\n",
      "Required data has been scraped from the page: ERNST & YOUNG\n",
      "Required data has been scraped from the page: LAS VEGAS SANDS CORP\n",
      "Required data has been scraped from the page: JIANGSU HENGRUI MEDICINE\n",
      "Required data has been scraped from the page: AMERICA MOVIL\n",
      "Required data has been scraped from the page: BAYERISCHE MOTOREN WERKE (BMW)\n",
      "Required data has been scraped from the page: CHINA VANKE\n",
      "Required data has been scraped from the page: DANONE\n",
      "Required data has been scraped from the page: GENERAL MOTORS COMPANY\n",
      "Required data has been scraped from the page: ABB\n",
      "Required data has been scraped from the page: WALGREENS BOOTS ALLIANCE\n",
      "Required data has been scraped from the page: BAIDU\n",
      "Required data has been scraped from the page: WAL-MART DE MEXICO\n",
      "Required data has been scraped from the page: ALDI\n",
      "Required data has been scraped from the page: MERCK KGAA O.N.\n",
      "Required data has been scraped from the page: BANK OF MONTREAL\n",
      "Required data has been scraped from the page: TC ENERGY CORPORATION\n",
      "Required data has been scraped from the page: VODAFONE GROUP\n",
      "Required data has been scraped from the page: CHINA SHENHUA ENERGY COMPANY\n",
      "Required data has been scraped from the page: INTERCONTINENTAL EXCHANGE\n",
      "Required data has been scraped from the page: UBER TECHNOLOGIES\n",
      "Required data has been scraped from the page: SUMITOMO MITSUI FINANCIAL GROUP\n",
      "Required data has been scraped from the page: HONDA MOTOR\n",
      "Required data has been scraped from the page: AIR PRODUCTS AND CHEMICALS\n",
      "Required data has been scraped from the page: QATAR NATIONAL BANK\n",
      "Required data has been scraped from the page: SHANGHAI PUDONG DEVELOPMENT BANK\n",
      "Required data has been scraped from the page: FOXCONN INDUSTRIAL INTERNET\n",
      "Required data has been scraped from the page: NORFOLK SOUTHERN CORPORATION\n",
      "Required data has been scraped from the page: CHUGAI PHARMACEUTICAL\n",
      "Required data has been scraped from the page: SUNCOR ENERGY\n",
      "Required data has been scraped from the page: PRUDENTIAL\n",
      "Required data has been scraped from the page: EQUINIX\n",
      "Required data has been scraped from the page: PHILLIPS 66\n",
      "Required data has been scraped from the page: MARRIOTT INTERNATIONAL\n",
      "Required data has been scraped from the page: AIRBNB\n",
      "Required data has been scraped from the page: WHATSAPP\n",
      "Required data has been scraped from the page: DBS GROUP HOLDINGS\n",
      "Required data has been scraped from the page: HCA HEALTHCARE\n",
      "Required data has been scraped from the page: MMC NORILSK NICKEL\n",
      "Required data has been scraped from the page: AMERICAN INTERNATIONAL GROUP (AIG)\n",
      "Required data has been scraped from the page: ILLUMINA\n",
      "Required data has been scraped from the page: ATLAS COPCO AB\n",
      "Required data has been scraped from the page: ICICI BANK\n",
      "Required data has been scraped from the page: EOG RESOURCES\n",
      "Required data has been scraped from the page: METLIFE\n",
      "Required data has been scraped from the page: ANZ BANKING GROUP\n",
      "Required data has been scraped from the page: EDWARDS LIFESCIENCES CORPORATION\n",
      "Required data has been scraped from the page: AON\n",
      "Required data has been scraped from the page: NINTENDO\n",
      "Required data has been scraped from the page: HUMANA\n",
      "Required data has been scraped from the page: RELX\n",
      "Required data has been scraped from the page: WASTE MANAGEMENT\n",
      "Required data has been scraped from the page: KINDER MORGAN\n",
      "Required data has been scraped from the page: ING GROUP\n",
      "Required data has been scraped from the page: BANCO SANTANDER BRASIL SA\n",
      "Required data has been scraped from the page: PINDUODUO\n",
      "Required data has been scraped from the page: CAPITAL ONE FINANCIAL CORPORATION\n",
      "Required data has been scraped from the page: DEUTSCHE POST AG\n",
      "Required data has been scraped from the page: SIEMENS HEALTHINEERS AG\n",
      "Required data has been scraped from the page: NETEASE\n",
      "Required data has been scraped from the page: PERNOD RICARD\n",
      "Required data has been scraped from the page: DUPONT DE NEMOURS\n",
      "Required data has been scraped from the page: EMERSON ELECTRIC COMPANY\n",
      "Required data has been scraped from the page: CHINA PACIFIC INSURANCE GROUP (CPIC)\n",
      "Required data has been scraped from the page: SHOPIFY\n",
      "Required data has been scraped from the page: AMERICAN ELECTRIC POWER COMPANY\n",
      "Required data has been scraped from the page: INTESA SANPAOLO\n",
      "Required data has been scraped from the page: JARDINE MATHESON HOLDINGS\n",
      "Required data has been scraped from the page: KIMBERLY-CLARK CORPORATION\n",
      "Required data has been scraped from the page: THE BANK OF NEW YORK MELLON CORPORATION\n",
      "Required data has been scraped from the page: UBS GROUP AG\n",
      "Required data has been scraped from the page: PING AN BANK\n",
      "Required data has been scraped from the page: BUDWEISER BREWING COMPANY APAC\n",
      "Required data has been scraped from the page: SANDS CHINA\n",
      "Required data has been scraped from the page: MOODY'S CORPORATION\n",
      "Required data has been scraped from the page: COMPAGNIE FINANCIERE RICHEMONT SA\n",
      "Required data has been scraped from the page: ACTIVISION BLIZZARD\n",
      "Required data has been scraped from the page: ORIENTAL LAND\n",
      "Required data has been scraped from the page: ANALOG DEVICES\n",
      "Required data has been scraped from the page: FIRST ABU DHABI BANK\n",
      "Required data has been scraped from the page: PUBLIC STORAGE\n",
      "Required data has been scraped from the page: SHIN-ETSU CHEMICAL\n",
      "Required data has been scraped from the page: BCE (Bell Canada Enterprises)\n",
      "Required data has been scraped from the page: KOTAK MAHINDRA BANK\n",
      "Required data has been scraped from the page: SUN HUNG KAI PROPERTIES\n",
      "Required data has been scraped from the page: AL RAJHI BANKING AND INVESTMENT CORPORATION\n",
      "Required data has been scraped from the page: KPMG\n",
      "Required data has been scraped from the page: EXELON CORPORATION\n",
      "Required data has been scraped from the page: L3HARRIS TECHNOLOGIES\n",
      "Required data has been scraped from the page: LAM RESEARCH CORPORATION\n",
      "Required data has been scraped from the page: ANHUI CONCH CEMENT COMPANY\n",
      "Required data has been scraped from the page: HANGZHOU HIKVISION DIGITAL TECHNOLOGY\n",
      "Required data has been scraped from the page: PHILIPS\n",
      "Required data has been scraped from the page: INVESTOR AB (PUBL)\n",
      "Required data has been scraped from the page: PICC GROUP (PEOPLE'S INSURANCE COMPANY (GROUP) OF CHINA)\n",
      "Required data has been scraped from the page: INFOSYS\n",
      "Required data has been scraped from the page: CHINA OVERSEAS LAND AND INVESTMENT\n",
      "Required data has been scraped from the page: ORSTED A/S\n",
      "Required data has been scraped from the page: SYSCO CORPORATION\n",
      "Required data has been scraped from the page: DAIICHI SANKYO COMPANY\n",
      "Required data has been scraped from the page: NATIONAL GRID TRANSCO\n",
      "Required data has been scraped from the page: BAXTER INTERNATIONAL\n",
      "Required data has been scraped from the page: BODEGAS ESMERALDA\n",
      "Required data has been scraped from the page: PROGRESSIVE CORPORATION\n",
      "Required data has been scraped from the page: MUNCHENER RUCKVERSICHERUNGS-GESELLSCHAFT\n",
      "Required data has been scraped from the page: DASSAULT SYSTEMES\n",
      "Required data has been scraped from the page: PTT PUBLIC COMPANY\n",
      "Required data has been scraped from the page: HONG KONG EXCHANGES AND CLEARING (HKEX)\n",
      "Required data has been scraped from the page: SEMPRA ENERGY\n",
      "Required data has been scraped from the page: CREDIT AGRICOLE\n",
      "Required data has been scraped from the page: CITIC SECURITIES COMPANY\n",
      "Required data has been scraped from the page: ROSS STORES\n",
      "Required data has been scraped from the page: STATE BANK OF INDIA\n",
      "Required data has been scraped from the page: HENKEL AG\n",
      "Required data has been scraped from the page: BARCLAYS\n",
      "Required data has been scraped from the page: HON HAI PRECISION INDUSTRY\n",
      "Required data has been scraped from the page: FERRARI\n",
      "Required data has been scraped from the page: GLENCORE\n",
      "Required data has been scraped from the page: DAIKIN INDUSTRIES\n",
      "Required data has been scraped from the page: IKEA\n",
      "Required data has been scraped from the page: HITACHI\n",
      "Required data has been scraped from the page: ECOPETROL\n",
      "Required data has been scraped from the page: REGENERON PHARMACEUTICALS\n",
      "Required data has been scraped from the page: ITC\n",
      "Required data has been scraped from the page: AUTODESK\n",
      "Required data has been scraped from the page: SINGAPORE TELECOMMUNICATIONS (SINGTEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required data has been scraped from the page: NIDEC CORPORATION\n",
      "Required data has been scraped from the page: COMPASS GROUP\n",
      "Required data has been scraped from the page: HANG SENG BANK\n",
      "Required data has been scraped from the page: MITSUBISHI CORPORATION\n",
      "Required data has been scraped from the page: KEURIG DR PEPPER\n",
      "Required data has been scraped from the page: DOW Inc\n",
      "Required data has been scraped from the page: CENTRAL JAPAN RAILWAY COMPANY\n",
      "Required data has been scraped from the page: MANULIFE FINANCIAL CORPORATION\n",
      "Required data has been scraped from the page: MURATA MANUFACTURING\n",
      "Required data has been scraped from the page: V.F. CORPORATION\n",
      "Required data has been scraped from the page: CHINA CITIC BANK CORPORATION\n",
      "Required data has been scraped from the page: DIDI CHUXING\n",
      "Required data has been scraped from the page: DOLLAR GENERAL CORPORATION\n",
      "Required data has been scraped from the page: FEDEX CORPORATION\n",
      "Required data has been scraped from the page: ENGIE\n",
      "Required data has been scraped from the page: ANGLO AMERICAN\n",
      "Required data has been scraped from the page: JAPAN TOBACCO\n",
      "Required data has been scraped from the page: KAO CORPORATION\n",
      "Required data has been scraped from the page: FOSHAN HAITIAN FLAVOURING AND FOOD COMPANY\n",
      "Required data has been scraped from the page: TOKIO MARINE HOLDINGS\n",
      "Required data has been scraped from the page: PT BANK RAKYAT INDONESIA (PERSERO) TBK\n",
      "Required data has been scraped from the page: SAIC MOTOR CORPORATION\n",
      "Required data has been scraped from the page: NATIONAL COMMERCIAL BANK\n",
      "Required data has been scraped from the page: CITIC\n",
      "Required data has been scraped from the page: NATWEST GROUP\n",
      "Required data has been scraped from the page: KRAFT HEINZ COMPANY\n",
      "Required data has been scraped from the page: EATON CORPORATION\n",
      "Required data has been scraped from the page: MARATHON PETROLEUM CORPORATION\n",
      "Required data has been scraped from the page: ORANGE S.A\n",
      "Required data has been scraped from the page: MIZUHO FINANCIAL GROUP\n",
      "Required data has been scraped from the page: CHINA MINSHENG BANKING\n",
      "Required data has been scraped from the page: AUDI AG\n",
      "Required data has been scraped from the page: VALERO ENERGY CORPORATION\n",
      "Required data has been scraped from the page: WORKDAY\n",
      "Required data has been scraped from the page: AFLAC\n",
      "Required data has been scraped from the page: NEWMONT CORPORATION\n",
      "Required data has been scraped from the page: JARDINE STRATEGIC HOLDINGS\n",
      "Required data has been scraped from the page: CHINA TOWER CORPORATION\n",
      "Required data has been scraped from the page: BANCO DO BRASIL\n",
      "Required data has been scraped from the page: BANCO BILBAO VIZCAYA ARGENTARIA\n",
      "Required data has been scraped from the page: CANADIAN NATURAL RESOURCES\n",
      "Required data has been scraped from the page: DELL TECHNOLOGIES\n",
      "Required data has been scraped from the page: JAPAN POST HOLDINGS\n",
      "Required data has been scraped from the page: DELTA AIR LINES\n",
      "Required data has been scraped from the page: OCCIDENTAL PETROLEUM CORPORATION\n",
      "Required data has been scraped from the page: CHINA EVERGRANDE GROUP\n",
      "Required data has been scraped from the page: FRESENIUS SE\n",
      "Required data has been scraped from the page: PRUDENTIAL FINANCIAL\n",
      "Required data has been scraped from the page: ROPER TECHNOLOGIES\n",
      "Required data has been scraped from the page: FORD MOTOR COMPANY\n",
      "Required data has been scraped from the page: CK HUTCHISON HOLDINGS\n",
      "Required data has been scraped from the page: CANADIAN IMPERIAL BANK OF COMMERCE\n",
      "Required data has been scraped from the page: BOC HONG KONG\n",
      "Required data has been scraped from the page: TELEFONICA SA\n",
      "Required data has been scraped from the page: MTR CORPORATION\n",
      "Required data has been scraped from the page: COUNTRY GARDEN HOLDINGS COMPANY\n",
      "Required data has been scraped from the page: CHINA RESOURCES LAND\n",
      "Required data has been scraped from the page: FANUC CORPORATION\n",
      "Required data has been scraped from the page: CONSTELLATION BRANDS\n",
      "Required data has been scraped from the page: ALLSTATE CORPORATION\n",
      "Required data has been scraped from the page: ALIMENTATION COUCHE-TARD\n",
      "Required data has been scraped from the page: JAPAN POST BANK\n",
      "Required data has been scraped from the page: HOYA CORPORATION\n",
      "Required data has been scraped from the page: OVERSEA-CHINESE BANKING CORPORATION\n",
      "Required data has been scraped from the page: SPACEX\n",
      "Required data has been scraped from the page: STRIPE\n",
      "Required data has been scraped from the page: THOMSON REUTERS CORPORATION\n",
      "Required data has been scraped from the page: BAJAJ FINANCE\n",
      "Required data has been scraped from the page: DENSO CORPORATION\n",
      "Required data has been scraped from the page: NXP SEMICONDUCTORS\n",
      "Required data has been scraped from the page: LONDON STOCK EXCHANGE GROUP\n",
      "Required data has been scraped from the page: AIRPORTS OF THAILAND\n",
      "Required data has been scraped from the page: THE TRAVELERS COMPANIES\n",
      "Required data has been scraped from the page: AB VOLVO\n",
      "Required data has been scraped from the page: XIAOMI CORPORATION\n",
      "Required data has been scraped from the page: AMADEUS IT GROUP\n",
      "Required data has been scraped from the page: H & M HENNES & MAURITZ AB\n",
      "Required data has been scraped from the page: CARNIVAL CORPORATION\n",
      "Required data has been scraped from the page: CANADIAN PACIFIC RAILWAY\n",
      "Required data has been scraped from the page: TOKYO ELECTRON LIMITED\n",
      "Required data has been scraped from the page: EDF (ELECTRICITE DE FRANCE)\n",
      "Required data has been scraped from the page: TRAFIGURA\n",
      "Required data has been scraped from the page: ITOCHU CORPORATION\n",
      "Required data has been scraped from the page: LINKEDIN\n",
      "Required data has been scraped from the page: ENERGY TRANSFER LP\n",
      "Required data has been scraped from the page: SURGUTNEFTEGAS\n",
      "Required data has been scraped from the page: NORDEA BANK ABP\n",
      "Required data has been scraped from the page: MACQUARIE GROUP\n",
      "Required data has been scraped from the page: VIVENDI\n",
      "Required data has been scraped from the page: WESFARMERS\n",
      "Required data has been scraped from the page: MONSTER BEVERAGE CORPORATION\n",
      "Required data has been scraped from the page: COGNIZANT TECHNOLOGY SOLUTIONS\n",
      "Required data has been scraped from the page: EAST JAPAN RAILWAY COMPANY\n",
      "Required data has been scraped from the page: KONE OYJ\n",
      "Required data has been scraped from the page: GALAXY ENTERTAINMENT GROUP\n",
      "Required data has been scraped from the page: CREDIT SUISSE GROUP\n",
      "Required data has been scraped from the page: UNICREDIT\n",
      "Required data has been scraped from the page: CHINA TELECOM CORPORATION\n",
      "Required data has been scraped from the page: WELLTOWER\n",
      "Required data has been scraped from the page: XCEL ENERGY\n",
      "Required data has been scraped from the page: HONG KONG AND CHINA GAS COMPANY\n",
      "Required data has been scraped from the page: TYSON FOODS\n",
      "Required data has been scraped from the page: BARRICK GOLD CORPORATION\n",
      "Required data has been scraped from the page: O'REILLY AUTOMOTIVE\n",
      "Required data has been scraped from the page: CONTEMPORARY AMPEREX TECHNOLOGY\n",
      "Required data has been scraped from the page: UNITED OVERSEAS BANK\n",
      "Required data has been scraped from the page: SOUTHERN COPPER CORPORATION\n",
      "Required data has been scraped from the page: TESCO\n",
      "Required data has been scraped from the page: BHARTI AIRTEL\n",
      "Required data has been scraped from the page: SEVEN & I HOLDINGS\n",
      "Required data has been scraped from the page: ASSICURAZIONI GENERALI\n",
      "Required data has been scraped from the page: AMPHENOL CORPORATION\n",
      "Required data has been scraped from the page: GENERAL MILLS\n",
      "Required data has been scraped from the page: FOMENTO ECONOMICO MEXICANOB. DE C.V. (FEMSA)\n",
      "CPU times: user 38.1 s, sys: 787 ms, total: 38.9 s\n",
      "Wall time: 13min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "companies_data = []\n",
    "for x in url_list:\n",
    "    companies_data.append(parse_companies_data(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, It took almost around 13 minutes. According as our previously expected time it was a bit fast. That's exactly the Magic of `Coding with Python`.\n",
    "\n",
    "Let's have a look on top 5 elements of the list if these are same list as we have got from top_5_companies_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_companies_data == companies_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we exactly got the same result.\n",
    "\n",
    "Now it's time to cearte a Pandas Dataframe from the above list of dictioanries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>CEO</th>\n",
       "      <th>Headquarter</th>\n",
       "      <th>Jan 2020 Rank</th>\n",
       "      <th>Jan 2020 Market Capitalization</th>\n",
       "      <th>July 2020 Rank</th>\n",
       "      <th>July 2020 Market Capitalization</th>\n",
       "      <th>Aug 2020 Rank</th>\n",
       "      <th>Aug 2020 Market Capitalization</th>\n",
       "      <th>Jan 2021 Rank</th>\n",
       "      <th>Jan 2021 Market Capitalization</th>\n",
       "      <th>Logo</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAUDI ARABIAN OIL COMPANY (Saudi Aramco)</td>\n",
       "      <td>Amin H. Al-Nasser</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>1</td>\n",
       "      <td>1,898.100 Billion USD</td>\n",
       "      <td>1</td>\n",
       "      <td>1,953.180 Billion USD</td>\n",
       "      <td>2</td>\n",
       "      <td>1,997.390 Billion USD</td>\n",
       "      <td>2</td>\n",
       "      <td>2,051.500 Billion USD</td>\n",
       "      <td>https://value.today/sites/default/files/styles...</td>\n",
       "      <td>https://www.saudiaramco.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APPLE</td>\n",
       "      <td>Tim Cook</td>\n",
       "      <td>USA</td>\n",
       "      <td>2</td>\n",
       "      <td>1,323.000 Billion USD</td>\n",
       "      <td>2</td>\n",
       "      <td>1,578.000 Billion USD</td>\n",
       "      <td>1</td>\n",
       "      <td>2,127.000 Billion USD</td>\n",
       "      <td>1</td>\n",
       "      <td>2,256.000 Billion USD</td>\n",
       "      <td>https://value.today/sites/default/files/styles...</td>\n",
       "      <td>https://www.apple.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MICROSOFT CORPORATION</td>\n",
       "      <td>Satya Nadella</td>\n",
       "      <td>USA</td>\n",
       "      <td>3</td>\n",
       "      <td>1,215.000 Billion USD</td>\n",
       "      <td>3</td>\n",
       "      <td>1,564.000 Billion USD</td>\n",
       "      <td>4</td>\n",
       "      <td>1,612.000 Billion USD</td>\n",
       "      <td>3</td>\n",
       "      <td>1,682.000 Billion USD</td>\n",
       "      <td>https://value.today/sites/default/files/styles...</td>\n",
       "      <td>https://www.microsoft.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALPHABET</td>\n",
       "      <td>Sundar Pichai</td>\n",
       "      <td>USA</td>\n",
       "      <td>4</td>\n",
       "      <td>943.897 Billion USD</td>\n",
       "      <td>5</td>\n",
       "      <td>1,002.000 Billion USD</td>\n",
       "      <td>5</td>\n",
       "      <td>1,073.000 Billion USD</td>\n",
       "      <td>5</td>\n",
       "      <td>1,185.000 Billion USD</td>\n",
       "      <td>https://value.today/sites/default/files/styles...</td>\n",
       "      <td>https://abc.xyz/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMAZON.COM</td>\n",
       "      <td>Jeff Bezos</td>\n",
       "      <td>USA</td>\n",
       "      <td>5</td>\n",
       "      <td>941.028 Billion USD</td>\n",
       "      <td>4</td>\n",
       "      <td>1,442.000 Billion USD</td>\n",
       "      <td>3</td>\n",
       "      <td>1,645.000 Billion USD</td>\n",
       "      <td>4</td>\n",
       "      <td>1,634.000 Billion USD</td>\n",
       "      <td>https://value.today/sites/default/files/styles...</td>\n",
       "      <td>https://www.amazon.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>SEVEN &amp; I HOLDINGS</td>\n",
       "      <td>Ryuichi Isaka</td>\n",
       "      <td>Japan</td>\n",
       "      <td>471</td>\n",
       "      <td>32.646 Billion USD</td>\n",
       "      <td>500</td>\n",
       "      <td>28.219 Billion USD</td>\n",
       "      <td>529</td>\n",
       "      <td>28.890 Billion USD</td>\n",
       "      <td>573</td>\n",
       "      <td>31.270 Billion USD</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.7andi.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>ASSICURAZIONI GENERALI</td>\n",
       "      <td>Philippe Donnet</td>\n",
       "      <td>Italy</td>\n",
       "      <td>472</td>\n",
       "      <td>32.607 Billion USD</td>\n",
       "      <td>596</td>\n",
       "      <td>24.205 Billion USD</td>\n",
       "      <td>657</td>\n",
       "      <td>23.968 Billion USD</td>\n",
       "      <td>674</td>\n",
       "      <td>27.258 Billion USD</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.generali.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>AMPHENOL CORPORATION</td>\n",
       "      <td>Richard Adam Norwitt</td>\n",
       "      <td>USA</td>\n",
       "      <td>473</td>\n",
       "      <td>32.525 Billion USD</td>\n",
       "      <td>498</td>\n",
       "      <td>28.367 Billion USD</td>\n",
       "      <td>452</td>\n",
       "      <td>32.296 Billion USD</td>\n",
       "      <td>442</td>\n",
       "      <td>39.121 Billion USD</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.amphenol.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>GENERAL MILLS</td>\n",
       "      <td>Jeff Harmening</td>\n",
       "      <td>USA</td>\n",
       "      <td>474</td>\n",
       "      <td>32.516 Billion USD</td>\n",
       "      <td>364</td>\n",
       "      <td>37.491 Billion USD</td>\n",
       "      <td>362</td>\n",
       "      <td>39.111 Billion USD</td>\n",
       "      <td>490</td>\n",
       "      <td>35.952 Billion USD</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.generalmills.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>FOMENTO ECONOMICO MEXICANOB. DE C.V. (FEMSA)</td>\n",
       "      <td>None</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>475</td>\n",
       "      <td>32.460 Billion USD</td>\n",
       "      <td>482</td>\n",
       "      <td>28.760 Billion USD</td>\n",
       "      <td>752</td>\n",
       "      <td>21.000 Billion USD</td>\n",
       "      <td>681</td>\n",
       "      <td>27.057 Billion USD</td>\n",
       "      <td>None</td>\n",
       "      <td>http://www.femsa.com/en/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Company Name                   CEO  \\\n",
       "0        SAUDI ARABIAN OIL COMPANY (Saudi Aramco)     Amin H. Al-Nasser   \n",
       "1                                           APPLE              Tim Cook   \n",
       "2                           MICROSOFT CORPORATION         Satya Nadella   \n",
       "3                                        ALPHABET         Sundar Pichai   \n",
       "4                                      AMAZON.COM            Jeff Bezos   \n",
       "..                                            ...                   ...   \n",
       "495                            SEVEN & I HOLDINGS         Ryuichi Isaka   \n",
       "496                        ASSICURAZIONI GENERALI       Philippe Donnet   \n",
       "497                          AMPHENOL CORPORATION  Richard Adam Norwitt   \n",
       "498                                 GENERAL MILLS        Jeff Harmening   \n",
       "499  FOMENTO ECONOMICO MEXICANOB. DE C.V. (FEMSA)                  None   \n",
       "\n",
       "      Headquarter Jan 2020 Rank Jan 2020 Market Capitalization July 2020 Rank  \\\n",
       "0    Saudi Arabia             1          1,898.100 Billion USD              1   \n",
       "1             USA             2          1,323.000 Billion USD              2   \n",
       "2             USA             3          1,215.000 Billion USD              3   \n",
       "3             USA             4            943.897 Billion USD              5   \n",
       "4             USA             5            941.028 Billion USD              4   \n",
       "..            ...           ...                            ...            ...   \n",
       "495         Japan           471             32.646 Billion USD            500   \n",
       "496         Italy           472             32.607 Billion USD            596   \n",
       "497           USA           473             32.525 Billion USD            498   \n",
       "498           USA           474             32.516 Billion USD            364   \n",
       "499        Mexico           475             32.460 Billion USD            482   \n",
       "\n",
       "    July 2020 Market Capitalization Aug 2020 Rank  \\\n",
       "0             1,953.180 Billion USD             2   \n",
       "1             1,578.000 Billion USD             1   \n",
       "2             1,564.000 Billion USD             4   \n",
       "3             1,002.000 Billion USD             5   \n",
       "4             1,442.000 Billion USD             3   \n",
       "..                              ...           ...   \n",
       "495              28.219 Billion USD           529   \n",
       "496              24.205 Billion USD           657   \n",
       "497              28.367 Billion USD           452   \n",
       "498              37.491 Billion USD           362   \n",
       "499              28.760 Billion USD           752   \n",
       "\n",
       "    Aug 2020 Market Capitalization Jan 2021 Rank  \\\n",
       "0            1,997.390 Billion USD             2   \n",
       "1            2,127.000 Billion USD             1   \n",
       "2            1,612.000 Billion USD             3   \n",
       "3            1,073.000 Billion USD             5   \n",
       "4            1,645.000 Billion USD             4   \n",
       "..                             ...           ...   \n",
       "495             28.890 Billion USD           573   \n",
       "496             23.968 Billion USD           674   \n",
       "497             32.296 Billion USD           442   \n",
       "498             39.111 Billion USD           490   \n",
       "499             21.000 Billion USD           681   \n",
       "\n",
       "    Jan 2021 Market Capitalization  \\\n",
       "0            2,051.500 Billion USD   \n",
       "1            2,256.000 Billion USD   \n",
       "2            1,682.000 Billion USD   \n",
       "3            1,185.000 Billion USD   \n",
       "4            1,634.000 Billion USD   \n",
       "..                             ...   \n",
       "495             31.270 Billion USD   \n",
       "496             27.258 Billion USD   \n",
       "497             39.121 Billion USD   \n",
       "498             35.952 Billion USD   \n",
       "499             27.057 Billion USD   \n",
       "\n",
       "                                                  Logo  \\\n",
       "0    https://value.today/sites/default/files/styles...   \n",
       "1    https://value.today/sites/default/files/styles...   \n",
       "2    https://value.today/sites/default/files/styles...   \n",
       "3    https://value.today/sites/default/files/styles...   \n",
       "4    https://value.today/sites/default/files/styles...   \n",
       "..                                                 ...   \n",
       "495                                               None   \n",
       "496                                               None   \n",
       "497                                               None   \n",
       "498                                               None   \n",
       "499                                               None   \n",
       "\n",
       "                               URL  \n",
       "0     https://www.saudiaramco.com/  \n",
       "1           https://www.apple.com/  \n",
       "2       https://www.microsoft.com/  \n",
       "3                 https://abc.xyz/  \n",
       "4          https://www.amazon.com/  \n",
       "..                             ...  \n",
       "495          https://www.7andi.com  \n",
       "496      https://www.generali.com/  \n",
       "497      https://www.amphenol.com/  \n",
       "498  https://www.generalmills.com/  \n",
       "499       http://www.femsa.com/en/  \n",
       "\n",
       "[500 rows x 13 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies_data_df = pd.DataFrame(companies_data)\n",
    "companies_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write this data into a `CSV` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_data_df.to_csv('companies_data.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, our CSV file has been created and we have done our work.\n",
    "\n",
    "It's time to save our entire notebook and upload both of our csv files to the jovian account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"thakubhai-007/web-scraping-final\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Uploading additional files...\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/thakubhai-007/web-scraping-final\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/thakubhai-007/web-scraping-final'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=\"web-scraping-final\", files=['companies_data.csv', 'companies.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "Finally we got the data we required and we successfully have transformed all the data into a `CSV` file.\n",
    "\n",
    "Here is very short description of our entire journey:\n",
    "* First of all we have installed and imported all the required library, but anyone can install and import any library anytime between the code cells whenever you need a your required library you can imstall and import the library.\n",
    "* We have make a HTTPs request with the `Requests` library to download the wesite into our notebook\n",
    "* We read the entire web-page using `BeautifulSoup` library, And then we started our main job `Finding the important data under the Tags`.\n",
    "* After getting all the required `tags` for extracting all the required data we than store all the data into a dictionary and then create a `DataFrame` using `Pandas` library.\n",
    "* And our work has finished for the web-page we have selected.\n",
    "\n",
    "Then we go much deeper into web-scraping, \n",
    "\n",
    "We wrote some resuable functions to get data from a web-page, this time we wrote functions because we are going to collect data from 500 different web pages for each single company. For example the [Saudi Arabian Oil Company (Saudi Aramco)](https://www.value.today/company/saudi-arabian-oil-company) and [APPLE](https://www.value.today/company/apple)\n",
    "\n",
    "* We wrote functions to get data from each single company's web-page\n",
    "* We wrote a parsing function to parse all the data into a list of dictionaries\n",
    "* Finally we wrote codes for to getting the data and parsing all the required data into a list of dictionaries\n",
    "* We make a pandas `DataFrame` for the collected data and then we create a `CSV` file with the data collected above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "I have made this project under the guidance of [Aakash N S](https://aakashns.medium.com/?source=collection_about-------------------------------------) and [Jovian team](https://blog.jovian.ai/about)\n",
    "\n",
    "\n",
    "Here is a list that can help you understand about some key points\n",
    "- There is a workshop on `Web-Scraping` by `Aakash N S` the link is [Let's Build a Python Web Scraping Project from Scratch | Hands-On Tutorial](https://www.youtube.com/watch?v=RKsLLG-bzEY&t=6677s)\n",
    "- You can read about all the libraries : [`Selenium`](https://www.selenium.dev/documentation/en/), [`Scrapy`](https://docs.scrapy.org/en/latest/) [`lxml`](https://lxml.de/) [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- You can also read some interesting blog post on web scraping written by my mates [Web Scraping Popular Movies using BeautifulSoup](https://blog.jovian.ai/web-scraping-popular-movies-using-beautifulsoup-5bab0852fee4) and [Web scraping tutorial for beginners](https://blog.jovian.ai/a-comprehensive-web-scraping-tutorial-385a2ac27107)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work\n",
    "\n",
    "\"There is always a lot to do with everything you have done previously\"\n",
    "\n",
    "The project we have made successfully will lead us to do Exploratory Analysis onto itself. Here is a list of future work that I will try further:\n",
    "- We can do our data science exploratory analysis on the same data set we have created so far.\n",
    "- There is still lots of data available on the same website that I can scrape, wiz: `Top companies data by Stock Exchange`, `Top companies data by Sector`, `Country wise Top companies data` and lots of way to scrape the data from the website: [value.today](https://www.value.today/)\n",
    "- I will make an another web scraping project using some different libraries that I have mentioned above, I'll soon try scraping using `Scrapy` and `Selenium`.\n",
    "\n",
    "\n",
    "### Hope you enjoyed the whole journey from scratch, Thanks for reading my notebook patiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"thakubhai-007/web-scraping-final\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Uploading additional files...\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/thakubhai-007/web-scraping-final\u001b[0m\n",
      "[jovian] Submitting assignment..\u001b[0m\n",
      "[jovian] Verify your submission at https://jovian.ai/learn/zero-to-data-analyst-bootcamp/assignment/project-1-web-scraping-with-python\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Let's submit our project to jovian plateform\n",
    "jovian.submit(assignment=\"zerotoanalyst-project1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}